# Event Sourcing Order Management - Deployment View Mermaid Diagrams

## ğŸ“‹ Tá»•ng quan Script Mermaid cho Deployment View

File nÃ y chá»©a cÃ¡c script Mermaid Ä‘á»ƒ váº½ diagram cho **Deployment View** cá»§a há»‡ thá»‘ng Event Sourcing Order Management. CÃ¡c diagram bao gá»“m kiáº¿n trÃºc triá»ƒn khai, network topology, CI/CD pipeline, monitoring stack vÃ  cÃ¡c chiáº¿n lÆ°á»£c deployment.

---

## ğŸ—ï¸ 1. Overall Deployment Architecture

### Script Mermaid
```mermaid
graph TB
    subgraph "External Users"
        USER[ğŸ‘¤ Users/Clients]
        ADMIN[ğŸ‘¨â€ğŸ’¼ Admin Dashboard]
    end
    
    subgraph "Load Balancer Layer"
        LB[ğŸ”„ Nginx Load Balancer<br/>ğŸ“¡ Port 80/443<br/>ğŸ“Š 2 cores, 4GB RAM]
    end
    
    subgraph "Application Layer"
        FE1[ğŸ–¥ï¸ Frontend Node 1<br/>âš›ï¸ Next.js Container<br/>ğŸ“¡ Port 3000<br/>ğŸ“Š 1 core, 2GB RAM]
        FE2[ğŸ–¥ï¸ Frontend Node 2<br/>âš›ï¸ Next.js Container<br/>ğŸ“¡ Port 3000<br/>ğŸ“Š 1 core, 2GB RAM]
        
        BE1[âš™ï¸ Backend Node 1<br/>ğŸŸ¢ Express.js Container<br/>ğŸ“¡ Port 3001<br/>ğŸ“Š 2 cores, 4GB RAM]
        BE2[âš™ï¸ Backend Node 2<br/>ğŸŸ¢ Express.js Container<br/>ğŸ“¡ Port 3001<br/>ğŸ“Š 2 cores, 4GB RAM]
    end
    
    subgraph "Data Layer"
        DB1[ğŸ—„ï¸ PostgreSQL Primary<br/>ğŸ“ Event Store<br/>ğŸ“¡ Port 5432<br/>ğŸ“Š 4 cores, 16GB RAM]
        DB2[ğŸ—„ï¸ PostgreSQL Replica<br/>ğŸ“– Read-only<br/>ğŸ“¡ Port 5432<br/>ğŸ“Š 2 cores, 8GB RAM]
        
        REDIS[âš¡ Redis Cache<br/>ğŸ’¾ Session & Projections<br/>ğŸ“¡ Port 6379<br/>ğŸ“Š 1 core, 8GB RAM]
    end
    
    subgraph "Infrastructure Layer"
        MON[ğŸ“Š Monitoring Stack<br/>ğŸ“ˆ Prometheus + Grafana<br/>ğŸ“¡ Port 9090/3000<br/>ğŸ“Š 2 cores, 8GB RAM]
        LOG[ğŸ“ Logging Stack<br/>ğŸ” ELK Stack<br/>ğŸ“¡ Port 5601<br/>ğŸ“Š 2 cores, 8GB RAM]
    end
    
    %% External connections
    USER -->|HTTPS/443| LB
    ADMIN -->|HTTPS/443| LB
    
    %% Load balancer to application layer
    LB -->|HTTP/3000| FE1
    LB -->|HTTP/3000| FE2
    LB -->|HTTP/3001| BE1
    LB -->|HTTP/3001| BE2
    
    %% Frontend to backend
    FE1 -->|API calls| BE1
    FE2 -->|API calls| BE2
    
    %% Backend to data layer
    BE1 -->|Write/Read| DB1
    BE2 -->|Write/Read| DB1
    BE1 -->|Read only| DB2
    BE2 -->|Read only| DB2
    BE1 -->|Cache ops| REDIS
    BE2 -->|Cache ops| REDIS
    
    %% Monitoring connections
    BE1 -.->|Metrics| MON
    BE2 -.->|Metrics| MON
    DB1 -.->|Metrics| MON
    DB2 -.->|Metrics| MON
    REDIS -.->|Metrics| MON
    
    %% Logging connections
    FE1 -.->|Logs| LOG
    FE2 -.->|Logs| LOG
    BE1 -.->|Logs| LOG
    BE2 -.->|Logs| LOG
    DB1 -.->|Logs| LOG
    DB2 -.->|Logs| LOG
    
    %% Database replication
    DB1 ==>|Streaming<br/>Replication| DB2
    
    %% Styling
    classDef userClass fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef lbClass fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef appClass fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef dataClass fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef infraClass fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    
    class USER,ADMIN userClass
    class LB lbClass
    class FE1,FE2,BE1,BE2 appClass
    class DB1,DB2,REDIS dataClass
    class MON,LOG infraClass
```

### MÃ´ táº£ sá»­ dá»¥ng:
- **Má»¥c Ä‘Ã­ch**: Hiá»ƒn thá»‹ tá»•ng quan kiáº¿n trÃºc triá»ƒn khai vá»›i táº¥t cáº£ cÃ¡c node vÃ  káº¿t ná»‘i
- **ThÃ nh pháº§n chÃ­nh**: Load balancer, application layer, data layer, infrastructure layer
- **ThÃ´ng tin hiá»ƒn thá»‹**: Ports, resource specifications, connection types
- **Sá»­ dá»¥ng**: Architecture overview, system documentation, stakeholder presentations

---

## ğŸŒ 2. Multi-Environment Deployment Strategy

### Script Mermaid
```mermaid
graph LR
    subgraph "Development Environment"
        DEV_LOCAL[ğŸ’» Local Development<br/>localhost:3000/3001<br/>ğŸ“Š Single machine]
        DEV_DB[ğŸ—„ï¸ Local PostgreSQL<br/>localhost:5432<br/>ğŸ“Š Development data]
    end
    
    subgraph "Staging Environment"
        STAGE_DOCKER[ğŸ³ Docker Compose<br/>staging.domain.com<br/>ğŸ“Š 2 replicas each]
        STAGE_LB[ğŸ”„ Nginx Container<br/>Load balancing]
        STAGE_DB[ğŸ—„ï¸ PostgreSQL Container<br/>Production-like data]
        STAGE_REDIS[âš¡ Redis Container<br/>Cache testing]
    end
    
    subgraph "Production Environment"
        PROD_K8S[â˜¸ï¸ Kubernetes Cluster<br/>production.domain.com<br/>ğŸ“Š 3+ replicas each]
        PROD_INGRESS[ğŸŒ Ingress Controller<br/>SSL termination]
        PROD_DB[ğŸ—„ï¸ PostgreSQL StatefulSet<br/>Master/Replica setup]
        PROD_REDIS[âš¡ Redis Deployment<br/>High availability]
        PROD_MON[ğŸ“Š Full Monitoring<br/>Prometheus/Grafana]
    end
    
    subgraph "Cloud-native Future"
        CLOUD_CDN[â˜ï¸ CDN + S3<br/>Static assets]
        CLOUD_MANAGED[â˜ï¸ Managed Services<br/>RDS, ElastiCache]
        CLOUD_SERVERLESS[âš¡ Serverless Functions<br/>Event processing]
        CLOUD_STREAM[ğŸŒŠ Event Streaming<br/>EventBridge/Event Hubs]
    end
    
    %% Environment progression
    DEV_LOCAL ==>|Code commit<br/>CI/CD trigger| STAGE_DOCKER
    STAGE_DOCKER ==>|Testing passed<br/>Manual approval| PROD_K8S
    PROD_K8S -.->|Future migration| CLOUD_CDN
    
    %% Internal connections
    DEV_LOCAL --- DEV_DB
    STAGE_LB --- STAGE_DOCKER
    STAGE_DOCKER --- STAGE_DB
    STAGE_DOCKER --- STAGE_REDIS
    
    PROD_INGRESS --- PROD_K8S
    PROD_K8S --- PROD_DB
    PROD_K8S --- PROD_REDIS
    PROD_K8S --- PROD_MON
    
    CLOUD_CDN --- CLOUD_MANAGED
    CLOUD_MANAGED --- CLOUD_SERVERLESS
    CLOUD_SERVERLESS --- CLOUD_STREAM
    
    %% Styling
    classDef devClass fill:#e3f2fd,stroke:#0d47a1,stroke-width:2px
    classDef stageClass fill:#fff8e1,stroke:#e65100,stroke-width:2px
    classDef prodClass fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef cloudClass fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    
    class DEV_LOCAL,DEV_DB devClass
    class STAGE_DOCKER,STAGE_LB,STAGE_DB,STAGE_REDIS stageClass
    class PROD_K8S,PROD_INGRESS,PROD_DB,PROD_REDIS,PROD_MON prodClass
    class CLOUD_CDN,CLOUD_MANAGED,CLOUD_SERVERLESS,CLOUD_STREAM cloudClass
```

### MÃ´ táº£ sá»­ dá»¥ng:
- **Má»¥c Ä‘Ã­ch**: Hiá»ƒn thá»‹ chiáº¿n lÆ°á»£c triá»ƒn khai qua cÃ¡c mÃ´i trÆ°á»ng
- **ThÃ nh pháº§n**: Development, Staging, Production, Cloud-native future
- **Flow**: Code progression tá»« development Ä‘áº¿n production
- **Sá»­ dá»¥ng**: DevOps planning, environment strategy, team alignment

---

## ğŸ”— 3. Network Communication & Security Topology

### Script Mermaid
```mermaid
graph TB
    subgraph "Public Internet (0.0.0.0/0)"
        INET[ğŸŒ Internet Users<br/>External Traffic]
    end
    
    subgraph "DMZ Zone (10.0.1.0/24)"
        LB[ğŸ”„ Load Balancer<br/>ğŸ“¡ 443/80<br/>ğŸ”’ SSL Termination]
        CDN[âš¡ CDN Endpoints<br/>ğŸ“¦ Static Assets<br/>ğŸ”’ DDoS Protection]
    end
    
    subgraph "Application Zone - Frontend (10.0.2.0/24)"
        FE[ğŸ–¥ï¸ Frontend Services<br/>ğŸ“¡ 3000<br/>ğŸ”’ No direct internet access]
    end
    
    subgraph "Application Zone - Backend (10.0.3.0/24)"
        BE[âš™ï¸ Backend Services<br/>ğŸ“¡ 3001<br/>ğŸ”’ API Gateway protected]
    end
    
    subgraph "Data Zone (10.0.4.0/24)"
        DB[ğŸ—„ï¸ Database Cluster<br/>ğŸ“¡ 5432<br/>ğŸ”’ Restricted access only]
        CACHE[âš¡ Cache Layer<br/>ğŸ“¡ 6379<br/>ğŸ”’ Internal network only]
    end
    
    subgraph "Management Zone (10.0.5.0/24)"
        MON[ğŸ“Š Monitoring<br/>ğŸ“¡ 9090<br/>ğŸ”’ Admin access only]
        LOG[ğŸ“ Logging<br/>ğŸ“¡ 5601<br/>ğŸ”’ Audit trail]
        JUMP[ğŸšª Jump Host<br/>ğŸ“¡ 22<br/>ğŸ”’ SSH bastion]
    end
    
    %% Public traffic flows
    INET -->|HTTPS/443<br/>ğŸ”’ TLS 1.3| LB
    INET -->|HTTP/80<br/>â¡ï¸ Redirect 443| CDN
    
    %% DMZ to application zones
    LB -->|HTTP/3000<br/>ğŸ”’ Internal TLS| FE
    LB -->|HTTP/3001<br/>ğŸ”’ Internal TLS| BE
    CDN -->|HTTP/3000<br/>ğŸ“¦ Assets only| FE
    
    %% Application layer communication
    FE -->|HTTP/3001<br/>ğŸ”’ Service mesh| BE
    
    %% Backend to data layer
    BE -->|TCP/5432<br/>ğŸ”’ Encrypted conn| DB
    BE -->|TCP/6379<br/>ğŸ”’ AUTH required| CACHE
    
    %% Management access
    BE -.->|HTTP/9090<br/>ğŸ“Š Metrics| MON
    FE -.->|HTTP/9090<br/>ğŸ“Š Metrics| MON
    DB -.->|HTTP/9090<br/>ğŸ“Š Metrics| MON
    
    BE -.->|Syslog/514<br/>ğŸ“ Structured logs| LOG
    FE -.->|Syslog/514<br/>ğŸ“ Access logs| LOG
    DB -.->|Syslog/514<br/>ğŸ“ Audit logs| LOG
    
    JUMP -.->|SSH/22<br/>ğŸ”’ Key-based auth| DB
    JUMP -.->|SSH/22<br/>ğŸ”’ Admin access| MON
    
    %% Firewall rules annotations
    LB -.->|ğŸš« Deny all other<br/>inbound traffic| INET
    FE -.->|ğŸš« No direct<br/>internet access| INET
    BE -.->|ğŸš« No direct<br/>internet access| INET
    DB -.->|ğŸš« Restricted access<br/>from Backend only| BE
    
    %% Styling
    classDef internetClass fill:#ffebee,stroke:#c62828,stroke-width:3px
    classDef dmzClass fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    classDef appClass fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    classDef dataClass fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
    classDef mgmtClass fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class INET internetClass
    class LB,CDN dmzClass
    class FE,BE appClass
    class DB,CACHE dataClass
    class MON,LOG,JUMP mgmtClass
```

### MÃ´ táº£ sá»­ dá»¥ng:
- **Má»¥c Ä‘Ã­ch**: Hiá»ƒn thá»‹ network topology vÃ  security boundaries
- **ThÃ nh pháº§n**: Network zones, firewall rules, communication protocols
- **Security**: Access controls, encryption, network segmentation
- **Sá»­ dá»¥ng**: Security planning, network design, compliance documentation

---

## ğŸš€ 4. CI/CD Pipeline Architecture

### Script Mermaid
```mermaid
graph LR
    subgraph "Source Control"
        GIT[ğŸ“š GitHub Repository<br/>ğŸ”€ Main/Develop branches<br/>ğŸ“ Pull requests]
    end
    
    subgraph "CI Pipeline"
        BUILD[ğŸ”¨ Build & Test<br/>âš¡ Node.js 18<br/>ğŸ§ª Unit/Integration tests]
        SCAN[ğŸ›¡ï¸ Security Scan<br/>ğŸ” Vulnerability check<br/>ğŸ“‹ Code quality]
        IMG[ğŸ“¦ Build Images<br/>ğŸ³ Docker build<br/>ğŸ·ï¸ Tag & version]
    end
    
    subgraph "Artifact Storage"
        REG[ğŸ“ Container Registry<br/>ğŸ³ GitHub Container Registry<br/>ğŸ·ï¸ ghcr.io/repo/image:tag]
    end
    
    subgraph "CD Pipeline"
        DEV[ğŸš€ Deploy to Dev<br/>ğŸ’» Local/Docker Compose<br/>âš¡ Instant deployment]
        STAGE[ğŸ§ª Deploy to Staging<br/>ğŸ³ Docker Swarm<br/>âœ… Smoke tests]
        APPROVAL[âœ‹ Manual Approval<br/>ğŸ‘¨â€ğŸ’¼ Product owner review<br/>ğŸ“‹ Release checklist]
        PROD[ğŸŒŸ Deploy to Production<br/>â˜¸ï¸ Kubernetes cluster<br/>ğŸ”„ Rolling update]
    end
    
    subgraph "Infrastructure"
        K8S[â˜¸ï¸ Kubernetes Cluster<br/>ğŸ·ï¸ Namespaces: dev/staging/prod<br/>âš–ï¸ Auto-scaling enabled]
        MON[ğŸ“Š Monitoring<br/>ğŸ“ˆ Prometheus/Grafana<br/>ğŸš¨ Alerting rules]
    end
    
    subgraph "Notifications"
        SLACK[ğŸ’¬ Slack Notifications<br/>âœ… Success alerts<br/>âŒ Failure alerts]
        EMAIL[ğŸ“§ Email Alerts<br/>ğŸ‘¨â€ğŸ’¼ Stakeholder updates<br/>ğŸ“Š Weekly reports]
    end
    
    %% CI Flow
    GIT -->|Webhook trigger<br/>ğŸ”” Push/PR events| BUILD
    BUILD -->|Tests passed<br/>âœ… Quality gates| SCAN
    SCAN -->|Security cleared<br/>ğŸ›¡ï¸ No vulnerabilities| IMG
    IMG -->|Image pushed<br/>ğŸ“¦ Artifact created| REG
    
    %% CD Flow
    REG -->|Auto deploy<br/>âš¡ Development| DEV
    DEV -->|Deploy on merge<br/>ğŸ”€ Main branch| STAGE
    STAGE -->|Manual gate<br/>âœ‹ Human approval| APPROVAL
    APPROVAL -->|Approved<br/>âœ… Go live| PROD
    
    %% Infrastructure
    PROD -->|Deployment<br/>â˜¸ï¸ Rolling update| K8S
    K8S -->|Health checks<br/>ğŸ’“ Readiness probes| MON
    
    %% Feedback loops
    MON -.->|Metrics<br/>ğŸ“Š Performance data| SLACK
    STAGE -.->|Test results<br/>ğŸ§ª Validation report| EMAIL
    PROD -.->|Deployment status<br/>ğŸš€ Go-live notification| SLACK
    
    %% Error flows
    BUILD -.->|Build failed<br/>âŒ Test failures| SLACK
    SCAN -.->|Security issues<br/>ğŸš¨ Vulnerabilities found| EMAIL
    PROD -.->|Deployment failed<br/>ğŸ’¥ Rollback triggered| SLACK
    
    %% Styling
    classDef sourceClass fill:#e3f2fd,stroke:#0d47a1,stroke-width:2px
    classDef ciClass fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef artifactClass fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef cdClass fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef infraClass fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    classDef notifClass fill:#e0f2f1,stroke:#00695c,stroke-width:2px
    
    class GIT sourceClass
    class BUILD,SCAN,IMG ciClass
    class REG artifactClass
    class DEV,STAGE,APPROVAL,PROD cdClass
    class K8S,MON infraClass
    class SLACK,EMAIL notifClass
```

### MÃ´ táº£ sá»­ dá»¥ng:
- **Má»¥c Ä‘Ã­ch**: Hiá»ƒn thá»‹ end-to-end CI/CD pipeline workflow
- **ThÃ nh pháº§n**: Source control, CI stages, CD environments, infrastructure
- **Flow**: Code to production deployment process
- **Sá»­ dá»¥ng**: DevOps documentation, process training, pipeline optimization

---

## ğŸ“Š 5. Monitoring & Observability Stack

### Script Mermaid
```mermaid
graph TB
    subgraph "Application Layer"
        APP[ğŸ–¥ï¸ Applications<br/>Frontend + Backend<br/>ğŸ“Š Custom metrics]
        DB[ğŸ—„ï¸ Database<br/>PostgreSQL cluster<br/>ğŸ“ˆ Query performance]
        CACHE[âš¡ Cache<br/>Redis cluster<br/>ğŸ’¾ Hit/miss ratios]
    end
    
    subgraph "Metrics Collection Layer"
        PROM[ğŸ“Š Prometheus<br/>ğŸ“ˆ Time-series DB<br/>â±ï¸ 15s scrape interval]
        NODE[ğŸ–¥ï¸ Node Exporter<br/>ğŸ“‹ System metrics<br/>ğŸ’» CPU/Memory/Disk]
        PG_EXP[ğŸ—„ï¸ PostgreSQL Exporter<br/>ğŸ“Š DB metrics<br/>ğŸ”— Connection pools]
        REDIS_EXP[âš¡ Redis Exporter<br/>ğŸ“ˆ Cache metrics<br/>âš¡ Performance stats]
    end
    
    subgraph "Logging Stack"
        LOG_AGG[ğŸ“ Log Aggregator<br/>ğŸŒŠ Fluentd/Fluent Bit<br/>ğŸ“¦ Log shipping]
        ES[ğŸ” Elasticsearch<br/>ğŸ“š Log storage<br/>ğŸ” Full-text search]
        KIBANA[ğŸ“Š Kibana<br/>ğŸ“ˆ Log visualization<br/>ğŸ” Query interface]
    end
    
    subgraph "Visualization & Alerting"
        GRAFANA[ğŸ“Š Grafana<br/>ğŸ“ˆ Dashboards<br/>ğŸ“Š Real-time charts]
        ALERT[ğŸš¨ Alertmanager<br/>ğŸ“¢ Notification routing<br/>ğŸ”„ Alert grouping]
    end
    
    subgraph "External Integrations"
        SLACK[ğŸ’¬ Slack<br/>ğŸ“¢ Team notifications<br/>ğŸ¤– Bot integrations]
        EMAIL[ğŸ“§ Email<br/>ğŸ“¨ Alert emails<br/>ğŸ“‹ Daily reports]
        PAGER[ğŸ“Ÿ PagerDuty<br/>ğŸš¨ On-call alerts<br/>ğŸ“ Escalation policies]
        WEBHOOK[ğŸ”— Webhooks<br/>ğŸ”„ Custom integrations<br/>âš¡ Real-time events]
    end
    
    %% Metrics flow
    APP -->|/metrics endpoint<br/>ğŸ“Š HTTP metrics| PROM
    DB -->|Database stats<br/>ğŸ“ˆ Performance data| PG_EXP
    CACHE -->|Cache statistics<br/>âš¡ Redis INFO| REDIS_EXP
    NODE -->|System metrics<br/>ğŸ’» Host statistics| PROM
    
    PG_EXP -->|Scraped metrics<br/>ğŸ“Š 30s interval| PROM
    REDIS_EXP -->|Cache metrics<br/>ğŸ“ˆ 30s interval| PROM
    
    %% Logging flow
    APP -->|Application logs<br/>ğŸ“ JSON format| LOG_AGG
    DB -->|Query logs<br/>ğŸ—„ï¸ Slow queries| LOG_AGG
    CACHE -->|Redis logs<br/>âš¡ Command logs| LOG_AGG
    
    LOG_AGG -->|Processed logs<br/>ğŸ“¦ Structured data| ES
    ES -->|Search & analytics<br/>ğŸ” Log analysis| KIBANA
    
    %% Visualization flow
    PROM -->|Query API<br/>ğŸ“Š PromQL queries| GRAFANA
    PROM -->|Alert rules<br/>ğŸš¨ Threshold checks| ALERT
    
    %% Notification flow
    ALERT -->|Critical alerts<br/>ğŸš¨ P0/P1 incidents| PAGER
    ALERT -->|Team notifications<br/>ğŸ’¬ Channel alerts| SLACK
    ALERT -->|Management alerts<br/>ğŸ“§ Executive summary| EMAIL
    ALERT -->|Custom webhooks<br/>ğŸ”— Integration hooks| WEBHOOK
    
    %% Cross-stack integration
    GRAFANA -.->|Dashboard links<br/>ğŸ”— Log correlation| KIBANA
    KIBANA -.->|Alert on logs<br/>ğŸ“ Log-based alerts| ALERT
    SLACK -.->|Interactive alerts<br/>ğŸ¤– Acknowledge/Resolve| ALERT
    
    %% Data flow styling
    APP -.->|Health checks<br/>ğŸ’“ Liveness probes| GRAFANA
    GRAFANA -.->|SLA monitoring<br/>ğŸ“Š Service quality| SLACK
    
    %% Styling
    classDef appClass fill:#e3f2fd,stroke:#0d47a1,stroke-width:2px
    classDef metricsClass fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef loggingClass fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef visualClass fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef notifClass fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    
    class APP,DB,CACHE appClass
    class PROM,NODE,PG_EXP,REDIS_EXP metricsClass
    class LOG_AGG,ES,KIBANA loggingClass
    class GRAFANA,ALERT visualClass
    class SLACK,EMAIL,PAGER,WEBHOOK notifClass
```

### MÃ´ táº£ sá»­ dá»¥ng:
- **Má»¥c Ä‘Ã­ch**: Hiá»ƒn thá»‹ complete observability stack architecture
- **ThÃ nh pháº§n**: Metrics, logs, visualization, alerting, notifications
- **Data flow**: Collection â†’ Processing â†’ Visualization â†’ Alerting
- **Sá»­ dá»¥ng**: Monitoring strategy, incident response, performance optimization

---

## ğŸ”„ 6. Container Orchestration & Deployment Strategies

### Script Mermaid
```mermaid
graph TB
    subgraph "Development Strategy"
        DEV_COMPOSE[ğŸ³ Docker Compose<br/>ğŸ“ docker-compose.dev.yml<br/>ğŸ”§ Hot reload enabled]
        DEV_VOLUMES[ğŸ“ Volume Mounts<br/>ğŸ”„ Source code sync<br/>âš¡ Instant updates]
    end
    
    subgraph "Staging Strategy"
        STAGE_SWARM[ğŸ Docker Swarm<br/>ğŸ“¦ Multi-container orchestration<br/>âš–ï¸ Load balancing]
        STAGE_SECRETS[ğŸ” Docker Secrets<br/>ğŸ—ï¸ Password management<br/>ğŸ”’ Encrypted storage]
    end
    
    subgraph "Production Strategy - Blue/Green"
        PROD_BLUE[ğŸ”µ Blue Environment<br/>â˜¸ï¸ Current production<br/>ğŸ‘¥ Active traffic]
        PROD_GREEN[ğŸŸ¢ Green Environment<br/>â˜¸ï¸ New deployment<br/>ğŸ§ª Testing phase]
        PROD_SWITCH[ğŸ”„ Traffic Switch<br/>âš¡ Instant cutover<br/>ğŸ”™ Quick rollback]
    end
    
    subgraph "Production Strategy - Canary"
        CANARY_OLD[ğŸ“¦ Stable Version<br/>ğŸ‘¥ 80% traffic<br/>âœ… Proven stable]
        CANARY_NEW[ğŸ†• New Version<br/>ğŸ‘¥ 20% traffic<br/>ğŸ“Š Performance monitoring]
        CANARY_CONTROL[ğŸ›ï¸ Traffic Control<br/>ğŸ“Š Gradual rollout<br/>ğŸš¨ Auto rollback]
    end
    
    subgraph "Kubernetes Infrastructure"
        K8S_MASTER[â˜¸ï¸ Master Nodes<br/>ğŸ›ï¸ Control plane<br/>ğŸ“‹ API server]
        K8S_WORKER[ğŸ’ª Worker Nodes<br/>ğŸƒâ€â™‚ï¸ Pod execution<br/>ğŸ“¦ Container runtime]
        K8S_INGRESS[ğŸŒ Ingress Controller<br/>ğŸ”„ Load balancing<br/>ğŸ”’ SSL termination]
        K8S_STORAGE[ğŸ’¾ Persistent Storage<br/>ğŸ“ StatefulSets<br/>ğŸ”„ Volume management]
    end
    
    subgraph "Auto-scaling & Health"
        HPA[ğŸ“ˆ Horizontal Pod Autoscaler<br/>âš–ï¸ CPU/Memory based<br/>ğŸ”„ Dynamic scaling]
        VPA[ğŸ“Š Vertical Pod Autoscaler<br/>ğŸ“ˆ Resource optimization<br/>ğŸ’¡ Right-sizing]
        HEALTH[ğŸ’“ Health Checks<br/>ğŸ©º Liveness probes<br/>âœ… Readiness probes]
        CIRCUIT[ğŸ”Œ Circuit Breakers<br/>ğŸ›¡ï¸ Failure isolation<br/>ğŸ”„ Auto recovery]
    end
    
    %% Development flow
    DEV_COMPOSE <--> DEV_VOLUMES
    
    %% Staging flow
    STAGE_SWARM --> STAGE_SECRETS
    
    %% Blue/Green flow
    PROD_BLUE <-->|ğŸ”„ Switch traffic| PROD_SWITCH
    PROD_GREEN <-->|ğŸ”„ Switch traffic| PROD_SWITCH
    PROD_SWITCH -.->|ğŸ”™ Rollback if needed| PROD_BLUE
    
    %% Canary flow
    CANARY_OLD <-->|ğŸ“Š Traffic split| CANARY_CONTROL
    CANARY_NEW <-->|ğŸ“Š Traffic split| CANARY_CONTROL
    CANARY_CONTROL -.->|ğŸš¨ Issues detected| CANARY_OLD
    
    %% Kubernetes architecture
    K8S_MASTER -->|ğŸ›ï¸ Orchestration| K8S_WORKER
    K8S_WORKER -->|ğŸŒ Service mesh| K8S_INGRESS
    K8S_WORKER -->|ğŸ’¾ Data persistence| K8S_STORAGE
    
    %% Auto-scaling integration
    K8S_WORKER <-->|ğŸ“ˆ Scale pods| HPA
    K8S_WORKER <-->|ğŸ“Š Optimize resources| VPA
    K8S_WORKER <-->|ğŸ’“ Monitor health| HEALTH
    K8S_WORKER <-->|ğŸ›¡ï¸ Handle failures| CIRCUIT
    
    %% Strategy progression
    DEV_COMPOSE ==>|ğŸš€ Promote| STAGE_SWARM
    STAGE_SWARM ==>|âœ… Validated| PROD_BLUE
    STAGE_SWARM ==>|âœ… Validated| CANARY_OLD
    
    %% Infrastructure dependencies
    PROD_BLUE -.->|â˜¸ï¸ Runs on| K8S_WORKER
    PROD_GREEN -.->|â˜¸ï¸ Runs on| K8S_WORKER
    CANARY_OLD -.->|â˜¸ï¸ Runs on| K8S_WORKER
    CANARY_NEW -.->|â˜¸ï¸ Runs on| K8S_WORKER
    
    %% Styling
    classDef devClass fill:#e3f2fd,stroke:#0d47a1,stroke-width:2px
    classDef stageClass fill:#fff8e1,stroke:#e65100,stroke-width:2px
    classDef blueGreenClass fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef canaryClass fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef k8sClass fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    classDef autoClass fill:#e0f2f1,stroke:#00695c,stroke-width:2px
    
    class DEV_COMPOSE,DEV_VOLUMES devClass
    class STAGE_SWARM,STAGE_SECRETS stageClass
    class PROD_BLUE,PROD_GREEN,PROD_SWITCH blueGreenClass
    class CANARY_OLD,CANARY_NEW,CANARY_CONTROL canaryClass
    class K8S_MASTER,K8S_WORKER,K8S_INGRESS,K8S_STORAGE k8sClass
    class HPA,VPA,HEALTH,CIRCUIT autoClass
```

### MÃ´ táº£ sá»­ dá»¥ng:
- **Má»¥c Ä‘Ã­ch**: Hiá»ƒn thá»‹ container orchestration vÃ  deployment strategies
- **ThÃ nh pháº§n**: Development, staging, production strategies, Kubernetes infrastructure
- **Strategies**: Blue/Green, Canary deployments, auto-scaling
- **Sá»­ dá»¥ng**: Deployment planning, risk mitigation, infrastructure design

---

## ğŸ”„ 7. Backup & Disaster Recovery Architecture

### Script Mermaid
```mermaid
graph TB
    subgraph "Production Environment"
        PROD_DB[ğŸ—„ï¸ PostgreSQL Primary<br/>ğŸ“ Live event store<br/>ğŸ’¾ 500GB SSD]
        PROD_REPLICA[ğŸ—„ï¸ PostgreSQL Replica<br/>ğŸ“– Read-only copy<br/>ğŸ”„ Streaming replication]
        PROD_REDIS[âš¡ Redis Primary<br/>ğŸ’¾ Cache & sessions<br/>ğŸ“¦ AOF persistence]
        PROD_CONFIG[âš™ï¸ Application Config<br/>â˜¸ï¸ Kubernetes manifests<br/>ğŸ”§ Environment settings]
    end
    
    subgraph "Backup Storage"
        LOCAL_BACKUP[ğŸ’¾ Local Backup<br/>ğŸ“ /backups directory<br/>â° Daily full backup]
        CLOUD_S3[â˜ï¸ AWS S3 Bucket<br/>ğŸŒ Cross-region replication<br/>ğŸ” Encrypted storage]
        CLOUD_GLACIER[ğŸ§Š AWS Glacier<br/>ğŸ“¦ Long-term archive<br/>ğŸ’° Cost-optimized]
    end
    
    subgraph "Backup Operations"
        FULL_BACKUP[ğŸ“¦ Full Database Backup<br/>ğŸ• Daily at 02:00 UTC<br/>ğŸ“Š pg_dump + compression]
        WAL_BACKUP[ğŸ“ WAL Archive<br/>â° Every 15 minutes<br/>ğŸ“ˆ Continuous backup]
        CONFIG_BACKUP[âš™ï¸ Configuration Backup<br/>ğŸ• Daily at 03:00 UTC<br/>ğŸ“‹ K8s manifests + configs]
        REDIS_BACKUP[âš¡ Redis Snapshot<br/>ğŸ•• Every 6 hours<br/>ğŸ’¾ RDB + AOF files]
    end
    
    subgraph "Disaster Recovery Site"
        DR_REGION[ğŸŒ DR Region<br/>ğŸ—ï¸ Standby infrastructure<br/>ğŸš€ Quick activation]
        DR_DB[ğŸ—„ï¸ Standby Database<br/>ğŸ“Š Last backup restored<br/>â±ï¸ RTO: 1 hour]
        DR_APP[ğŸ–¥ï¸ Standby Applications<br/>â˜¸ï¸ Scaled-down cluster<br/>ğŸ”„ Auto-scaling ready]
        DR_MONITOR[ğŸ“Š DR Monitoring<br/>ğŸš¨ Health checks<br/>ğŸ“± Alert on failures]
    end
    
    subgraph "Recovery Procedures"
        RTO_TARGET[â±ï¸ RTO Target<br/>ğŸ¯ 1 hour recovery<br/>ğŸ“Š 99.9% uptime SLA]
        RPO_TARGET[ğŸ“Š RPO Target<br/>ğŸ¯ 15 minutes data loss<br/>ğŸ“ˆ WAL-based recovery]
        AUTO_FAILOVER[ğŸ”„ Auto Failover<br/>ğŸ¤– Automated detection<br/>âš¡ Instant switchover]
        MANUAL_RECOVERY[ğŸ‘¨â€ğŸ’» Manual Recovery<br/>ğŸ“‹ Step-by-step procedures<br/>âœ… Validation checklist]
    end
    
    %% Backup flows
    PROD_DB -->|ğŸ“¦ Daily backup<br/>ğŸ• 02:00 UTC| FULL_BACKUP
    PROD_DB -->|ğŸ“ WAL streaming<br/>â° Every 15min| WAL_BACKUP
    PROD_REPLICA -->|ğŸ“Š Replica backup<br/>ğŸ”„ Offload primary| FULL_BACKUP
    PROD_REDIS -->|ğŸ’¾ RDB snapshot<br/>ğŸ•• Every 6h| REDIS_BACKUP
    PROD_CONFIG -->|âš™ï¸ Config export<br/>ğŸ• Daily| CONFIG_BACKUP
    
    %% Storage flows
    FULL_BACKUP -->|ğŸ“ Local storage<br/>ğŸ’¾ Immediate access| LOCAL_BACKUP
    WAL_BACKUP -->|ğŸ“ Local archive<br/>ğŸ“ WAL files| LOCAL_BACKUP
    CONFIG_BACKUP -->|ğŸ“ Local storage<br/>âš™ï¸ Manifest files| LOCAL_BACKUP
    REDIS_BACKUP -->|ğŸ“ Local storage<br/>ğŸ’¾ Redis dumps| LOCAL_BACKUP
    
    LOCAL_BACKUP -->|â˜ï¸ Cloud sync<br/>ğŸ”„ Hourly upload| CLOUD_S3
    CLOUD_S3 -->|ğŸ§Š Archive transition<br/>ğŸ“… 30 days retention| CLOUD_GLACIER
    
    %% DR flows
    CLOUD_S3 -->|ğŸŒ Cross-region<br/>ğŸ”„ Replication| DR_REGION
    DR_REGION -->|ğŸ—„ï¸ Restore database<br/>ğŸ“Š Latest backup| DR_DB
    DR_REGION -->|ğŸ–¥ï¸ Deploy apps<br/>â˜¸ï¸ Kubernetes| DR_APP
    DR_REGION -->|ğŸ“Š Monitor status<br/>ğŸš¨ Health checks| DR_MONITOR
    
    %% Recovery procedures
    DR_MONITOR -->|â±ï¸ Meet RTO<br/>ğŸ¯ 1 hour target| RTO_TARGET
    WAL_BACKUP -->|ğŸ“Š Meet RPO<br/>ğŸ¯ 15min target| RPO_TARGET
    DR_MONITOR -->|ğŸ¤– Auto detection<br/>âš¡ Failover trigger| AUTO_FAILOVER
    DR_DB -->|ğŸ‘¨â€ğŸ’» Manual steps<br/>ğŸ“‹ Recovery guide| MANUAL_RECOVERY
    
    %% Testing and validation
    DR_DB -.->|ğŸ§ª Monthly test<br/>âœ… Restore validation| FULL_BACKUP
    DR_APP -.->|ğŸ§ª Quarterly DR drill<br/>ğŸ“Š Full system test| DR_MONITOR
    AUTO_FAILOVER -.->|ğŸ§ª Semi-annual test<br/>âš¡ Failover validation| DR_REGION
    
    %% Styling
    classDef prodClass fill:#e3f2fd,stroke:#0d47a1,stroke-width:2px
    classDef backupClass fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef storageClass fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef drClass fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef recoveryClass fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    
    class PROD_DB,PROD_REPLICA,PROD_REDIS,PROD_CONFIG prodClass
    class FULL_BACKUP,WAL_BACKUP,CONFIG_BACKUP,REDIS_BACKUP backupClass
    class LOCAL_BACKUP,CLOUD_S3,CLOUD_GLACIER storageClass
    class DR_REGION,DR_DB,DR_APP,DR_MONITOR drClass
    class RTO_TARGET,RPO_TARGET,AUTO_FAILOVER,MANUAL_RECOVERY recoveryClass
```

### MÃ´ táº£ sá»­ dá»¥ng:
- **Má»¥c Ä‘Ã­ch**: Hiá»ƒn thá»‹ comprehensive backup vÃ  disaster recovery strategy
- **ThÃ nh pháº§n**: Production systems, backup operations, storage layers, DR site
- **Recovery**: RTO/RPO targets, automated and manual procedures
- **Sá»­ dá»¥ng**: Business continuity planning, compliance documentation, DR testing

---

## ğŸ¯ 8. Resource Allocation & Scaling Architecture

### Script Mermaid
```mermaid
graph TB
    subgraph "Resource Specifications"
        LOAD_BALANCER[ğŸ”„ Load Balancer<br/>ğŸ“Š 2 cores, 4GB RAM<br/>ğŸ“¡ 1Gbps network<br/>ğŸ’¾ 20GB SSD]
        FRONTEND[ğŸ–¥ï¸ Frontend Pods<br/>ğŸ“Š 1 core, 2GB RAM each<br/>ğŸ”¢ 2-5 replicas<br/>ğŸ“¦ 10GB storage]
        BACKEND[âš™ï¸ Backend Pods<br/>ğŸ“Š 2 cores, 4GB RAM each<br/>ğŸ”¢ 2-5 replicas<br/>ğŸ“¦ 10GB storage]
        DATABASE[ğŸ—„ï¸ Database Primary<br/>ğŸ“Š 4 cores, 16GB RAM<br/>ğŸ’¾ 500GB SSD<br/>ğŸ”Œ High-speed network]
        REPLICA[ğŸ—„ï¸ Database Replica<br/>ğŸ“Š 2 cores, 8GB RAM each<br/>ğŸ”¢ 2 instances<br/>ğŸ’¾ 500GB SSD sync]
        REDIS_CACHE[âš¡ Redis Cache<br/>ğŸ“Š 1 core, 8GB RAM<br/>ğŸ’¾ 50GB persistence<br/>ğŸ”„ Memory optimization]
        MONITORING[ğŸ“Š Monitoring Stack<br/>ğŸ“Š 2 cores, 8GB RAM<br/>ğŸ’¾ 200GB metrics/logs<br/>ğŸ”— Full cluster access]
    end
    
    subgraph "Auto-scaling Policies"
        HPA_FRONTEND[ğŸ“ˆ Frontend HPA<br/>ğŸ¯ Target: 70% CPU<br/>ğŸ“Š Min: 2, Max: 10<br/>â±ï¸ Scale up: 30s]
        HPA_BACKEND[ğŸ“ˆ Backend HPA<br/>ğŸ¯ Target: 80% CPU<br/>ğŸ“Š Min: 2, Max: 8<br/>â±ï¸ Scale up: 30s]
        VPA_POLICIES[ğŸ“Š VPA Recommendations<br/>ğŸ’¡ Resource right-sizing<br/>ğŸ“ˆ Historical analysis<br/>ğŸ”„ Auto-adjustment]
        CLUSTER_AUTO[â˜¸ï¸ Cluster Autoscaler<br/>ğŸ—ï¸ Node pool scaling<br/>ğŸ“Š Resource requests<br/>ğŸ’° Cost optimization]
    end
    
    subgraph "Performance Thresholds"
        CPU_THRESHOLDS[ğŸ’» CPU Thresholds<br/>ğŸŸ¢ Normal: <70%<br/>ğŸŸ¡ Warning: 70-85%<br/>ğŸ”´ Critical: >85%]
        MEMORY_THRESHOLDS[ğŸ§  Memory Thresholds<br/>ğŸŸ¢ Normal: <80%<br/>ğŸŸ¡ Warning: 80-90%<br/>ğŸ”´ Critical: >90%]
        NETWORK_THRESHOLDS[ğŸ“¡ Network Thresholds<br/>ğŸŸ¢ Normal: <500Mbps<br/>ğŸŸ¡ Warning: 500-800Mbps<br/>ğŸ”´ Critical: >800Mbps]
        DISK_THRESHOLDS[ğŸ’¾ Disk Thresholds<br/>ğŸŸ¢ Normal: <75%<br/>ğŸŸ¡ Warning: 75-90%<br/>ğŸ”´ Critical: >90%]
    end
    
    subgraph "Scaling Triggers"
        TRAFFIC_SPIKE[ğŸ“ˆ Traffic Spike<br/>ğŸš€ Sudden load increase<br/>âš¡ Auto-scale trigger<br/>ğŸ“Š Response time SLA]
        RESOURCE_PRESSURE[ğŸ’¥ Resource Pressure<br/>ğŸ“Š High CPU/Memory<br/>ğŸ”„ Scale-out trigger<br/>âš–ï¸ Load distribution]
        SCHEDULED_SCALE[â° Scheduled Scaling<br/>ğŸ“… Predictable patterns<br/>ğŸ¢ Business hours<br/>ğŸ“Š Proactive scaling]
        COST_OPTIMIZATION[ğŸ’° Cost Optimization<br/>ğŸ“‰ Low usage periods<br/>ğŸ”½ Scale-down trigger<br/>ğŸ’¡ Resource efficiency]
    end
    
    subgraph "Scaling Actions"
        SCALE_OUT[ğŸ“ˆ Scale Out<br/>â• Add more pods<br/>âš–ï¸ Distribute load<br/>â±ï¸ Horizontal scaling]
        SCALE_UP[ğŸ“Š Scale Up<br/>â¬†ï¸ Increase resources<br/>ğŸ’ª More CPU/RAM<br/>â±ï¸ Vertical scaling]
        SCALE_DOWN[ğŸ“‰ Scale Down<br/>â– Remove pods<br/>ğŸ’° Reduce costs<br/>â±ï¸ Graceful shutdown]
        SCALE_IN[ğŸ“Š Scale In<br/>â¬‡ï¸ Decrease resources<br/>ğŸ’¡ Right-sizing<br/>â±ï¸ Resource optimization]
    end
    
    %% Resource allocation flows
    LOAD_BALANCER -.->|ğŸ”„ Routes traffic| FRONTEND
    LOAD_BALANCER -.->|ğŸ”„ Routes traffic| BACKEND
    FRONTEND -.->|ğŸ“ API calls| BACKEND
    BACKEND -.->|ğŸ“Š Read/Write| DATABASE
    BACKEND -.->|ğŸ“– Read queries| REPLICA
    BACKEND -.->|âš¡ Cache ops| REDIS_CACHE
    MONITORING -.->|ğŸ“Š Observes all| DATABASE
    
    %% Auto-scaling relationships
    FRONTEND <-->|ğŸ“ˆ Scales based on| HPA_FRONTEND
    BACKEND <-->|ğŸ“ˆ Scales based on| HPA_BACKEND
    FRONTEND <-->|ğŸ’¡ Optimizes| VPA_POLICIES
    BACKEND <-->|ğŸ’¡ Optimizes| VPA_POLICIES
    CLUSTER_AUTO -.->|ğŸ—ï¸ Provisions nodes| FRONTEND
    CLUSTER_AUTO -.->|ğŸ—ï¸ Provisions nodes| BACKEND
    
    %% Threshold monitoring
    FRONTEND -.->|ğŸ“Š CPU monitoring| CPU_THRESHOLDS
    BACKEND -.->|ğŸ§  Memory monitoring| MEMORY_THRESHOLDS
    DATABASE -.->|ğŸ“¡ Network monitoring| NETWORK_THRESHOLDS
    REPLICA -.->|ğŸ’¾ Disk monitoring| DISK_THRESHOLDS
    
    %% Scaling triggers
    TRAFFIC_SPIKE -->|ğŸš€ Triggers| SCALE_OUT
    RESOURCE_PRESSURE -->|ğŸ’¥ Triggers| SCALE_UP
    SCHEDULED_SCALE -->|â° Triggers| SCALE_OUT
    COST_OPTIMIZATION -->|ğŸ’° Triggers| SCALE_DOWN
    
    %% Scaling actions
    HPA_FRONTEND -->|ğŸ“ˆ Executes| SCALE_OUT
    HPA_BACKEND -->|ğŸ“ˆ Executes| SCALE_OUT
    VPA_POLICIES -->|ğŸ“Š Executes| SCALE_UP
    VPA_POLICIES -->|ğŸ“Š Executes| SCALE_IN
    
    %% Feedback loops
    SCALE_OUT -.->|ğŸ“Š Monitors impact| CPU_THRESHOLDS
    SCALE_UP -.->|ğŸ“Š Monitors impact| MEMORY_THRESHOLDS
    SCALE_DOWN -.->|ğŸ“Š Validates safety| TRAFFIC_SPIKE
    SCALE_IN -.->|ğŸ“Š Validates safety| RESOURCE_PRESSURE
    
    %% Styling
    classDef resourceClass fill:#e3f2fd,stroke:#0d47a1,stroke-width:2px
    classDef scalingClass fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef thresholdClass fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef triggerClass fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef actionClass fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    
    class LOAD_BALANCER,FRONTEND,BACKEND,DATABASE,REPLICA,REDIS_CACHE,MONITORING resourceClass
    class HPA_FRONTEND,HPA_BACKEND,VPA_POLICIES,CLUSTER_AUTO scalingClass
    class CPU_THRESHOLDS,MEMORY_THRESHOLDS,NETWORK_THRESHOLDS,DISK_THRESHOLDS thresholdClass
    class TRAFFIC_SPIKE,RESOURCE_PRESSURE,SCHEDULED_SCALE,COST_OPTIMIZATION triggerClass
    class SCALE_OUT,SCALE_UP,SCALE_DOWN,SCALE_IN actionClass
```

### MÃ´ táº£ sá»­ dá»¥ng:
- **Má»¥c Ä‘Ã­ch**: Hiá»ƒn thá»‹ resource allocation vÃ  auto-scaling architecture
- **ThÃ nh pháº§n**: Resource specs, scaling policies, thresholds, triggers, actions
- **Scaling**: Horizontal vÃ  vertical scaling strategies
- **Sá»­ dá»¥ng**: Capacity planning, cost optimization, performance tuning

---

## ğŸ“‹ HÆ°á»›ng dáº«n sá»­ dá»¥ng Mermaid Scripts

### CÃ¡ch sá»­ dá»¥ng cÃ¡c script:

1. **Copy script** tá»« cÃ¡c section trÃªn
2. **Paste vÃ o Mermaid editor**:
   - [Mermaid Live Editor](https://mermaid.live)
   - GitHub/GitLab (markdown files)
   - VS Code vá»›i Mermaid extension
   - Confluence, Notion, hay cÃ¡c tools khÃ¡c há»— trá»£ Mermaid

3. **Customize theo needs**:
   - Thay Ä‘á»•i colors trong `classDef` statements
   - Äiá»u chá»‰nh labels vÃ  descriptions
   - ThÃªm/bá»›t components theo architecture requirements
   - Modify connections vÃ  data flows

### Customization Examples:

#### Thay Ä‘á»•i mÃ u sáº¯c:
```mermaid
classDef myCustomClass fill:#your-color,stroke:#border-color,stroke-width:2px
class NODE1,NODE2 myCustomClass
```

#### ThÃªm styling cho connections:
```mermaid
A -->|Label text| B
A -.->|Dotted line| C
A ==>|Thick line| D
```

#### Táº¡o subgraphs má»›i:
```mermaid
subgraph "Your Custom Group"
    COMPONENT[Your Component<br/>Description<br/>Specs]
end
```

### Best Practices:

1. **Consistent naming**: Sá»­ dá»¥ng naming convention nháº¥t quÃ¡n
2. **Clear labels**: Labels ngáº¯n gá»n nhÆ°ng Ä‘áº§y Ä‘á»§ thÃ´ng tin
3. **Logical grouping**: Group related components trong subgraphs
4. **Color coding**: Sá»­ dá»¥ng colors Ä‘á»ƒ phÃ¢n biá»‡t cÃ¡c loáº¡i components
5. **Documentation**: Include mÃ´ táº£ vÃ  context cho má»—i diagram

### Integration vá»›i documentation:

- **Architecture docs**: Embed trong technical specifications
- **Presentations**: Export as images cho slides
- **Training materials**: Visual aids cho team onboarding
- **Compliance**: Document infrastructure cho audits
- **Planning**: Visualize proposed changes vÃ  improvements

---

**Total Scripts**: 8 comprehensive Mermaid diagrams covering all aspects cá»§a Deployment View architecture, tá»« overall infrastructure Ä‘áº¿n detailed scaling policies vÃ  disaster recovery procedures.
